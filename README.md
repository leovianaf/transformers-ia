# Projeto de Processamento de Linguagem Natural com Transformers

## Informa√ß√µes:

* Andreza Maria Coutinho Falc√£o - andreza.mcfalcao@ufrpe.br
* Beatriz Pereira da Silva - beatriz.pereiras@ufrpe.br
* Leonardo da Silva Viana Filho - leonardo.vianafilho@ufrpe.br
* Victor Schimitz Donato Cavalcanti - victor.schmitz@ufrpe.br

## Objetivo

Este projeto demonstra como utilizar a biblioteca `transformers` para realizar v√°rias tarefas de Processamento de Linguagem Natural (NLP) com modelos pr√©-treinados. As tarefas abordadas incluem:

1. An√°lise de Sentimento
2. Classifica√ß√£o de Texto
3. Classifica√ß√£o Zero-Shot
4. Resumo de Texto
5. Perguntas e Respostas (Question Answering)

## Requisitos

- Python 3.7 ou superior
- Biblioteca `transformers`
- Biblioteca `torch`

Voc√™ pode instalar as bibliotecas necess√°rias usando pip:

```bash
pip install transformers torch
```

## Descri√ß√£o

Este projeto utiliza a biblioteca `transformers` para aplicar modelos pr√©-treinados em tarefas espec√≠ficas de NLP. A biblioteca `transformers` da Hugging Face fornece uma interface f√°cil para diversos modelos de linguagem, como BERT, DistilBERT, BART, e mais.

### Tarefas Demonstradas

1. **An√°lise de Sentimento:**
   Utiliza um modelo pr√©-treinado para determinar o sentimento (positivo ou negativo) de um texto.

2. **Classifica√ß√£o de Texto:**
   Usa um modelo pr√©-treinado para classificar textos em categorias espec√≠ficas.

3. **Classifica√ß√£o Zero-Shot:**
   Aplica uma abordagem de zero-shot para classificar textos em categorias n√£o vistas durante o treinamento.

4. **Resumo de Texto:**
   Gera um resumo para um texto longo usando um modelo de sumariza√ß√£o.

5. **Perguntas e Respostas:**
   Responde perguntas com base em um contexto fornecido usando um modelo de QA.

## Passo a Passo

### 1. An√°lise de Sentimento

```python
from transformers import pipeline

# Cria um pipeline de an√°lise de sentimento
classifier = pipeline("sentiment-analysis")

# Aplica o pipeline a um texto
result = classifier("I loved Star Wars so much!")
print(result)
```

**Descri√ß√£o:** Utiliza um modelo pr√©-treinado para classificar o sentimento do texto como positivo ou negativo.

### 2. Classifica√ß√£o de Texto

```python
from transformers import pipeline

# Cria um pipeline de classifica√ß√£o de texto com um modelo BERT
classifier_txt = pipeline("text-classification", model="bert-base-uncased")

# Textos para classifica√ß√£o
texts_classif = [
    "I love this movie! It's fantastic.",
    "I hate this movie. It's terrible."
]

# Obt√©m os resultados da classifica√ß√£o
results = classifier_txt(texts_classif)
print(results)
```

**Descri√ß√£o:** Classifica os textos em categorias espec√≠ficas usando um modelo pr√©-treinado BERT.

### 3. Classifica√ß√£o Zero-Shot

```python
from transformers import pipeline

# Cria um pipeline de classifica√ß√£o zero-shot com um modelo BART
classifier_multi = pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

# Textos para classifica√ß√£o
texts_classif = [
    "I love this movie! It's fantastic.",
    "The political situation is very tense.",
    "The new technology in AI is groundbreaking.",
    "The recent sports event was thrilling."
]

# Labels (categorias) que voc√™ deseja classificar
candidate_labels = ["entertainment", "politics", "technology", "sports"]

# Aplica a classifica√ß√£o zero-shot
results = [classifier_multi(text, candidate_labels) for text in texts_classif]

# Imprime os resultados
for result in results:
    print(result)
```

**Descri√ß√£o:** Classifica textos em categorias n√£o vistas durante o treinamento usando uma abordagem zero-shot com BART.

### 4. Resumo de Texto

```python
from transformers import pipeline

# Cria um pipeline de sumariza√ß√£o
sumarizacao = pipeline("summarization")

# Texto para sumariza√ß√£o
text = "Transformer models are a type of deep neural network, similar to other types of neural networks such as Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs). The core of Transformer models is the self-attention mechanism, which allows the model to view different parts of a sequence simultaneously and determine the importance of each part. To understand the self-attention mechanism more clearly, imagine you are in a noisy room trying to hear a specific voice. Your brain can automatically focus on that specific voice while trying to ignore other voices. The self-attention mechanism works in a similar way."

# Obt√©m o resumo
result = sumarizacao(text)
print(result)
```

**Descri√ß√£o:** Gera um resumo de um texto longo utilizando um modelo de sumariza√ß√£o.

### 5. Perguntas e Respostas

```python
from transformers import pipeline

# Cria um pipeline de perguntas e respostas
question_answerer = pipeline("question-answering")

# Contexto e pergunta
context = "ü§ó Transformers is backed by the three most popular deep learning libraries ‚Äî Jax, PyTorch, and TensorFlow ‚Äî with a seamless integration between them."
question = "Which deep learning libraries back ü§ó Transformers?"

# Obt√©m a resposta
result = question_answerer(question=question, context=context)
print(result)
```

**Descri√ß√£o:** Responde a perguntas com base em um contexto fornecido usando um modelo de QA.

## Refer√™ncias

- **Hugging Face Transformers:** A biblioteca `transformers` fornece uma interface f√°cil para diversos modelos de linguagem. Para mais informa√ß√µes, consulte a [documenta√ß√£o oficial](https://huggingface.co/transformers/).
  
- **Paper:** "Attention is All You Need" (2017) - Introduz a arquitetura Transformer, que revolucionou o campo do Processamento de Linguagem Natural. Voc√™ pode ler o paper completo [aqui](https://arxiv.org/abs/1706.03762).
